{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load JSON data\n",
    "file_path = \"final_training_data.json\"  # Update with correct file path\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Select features\n",
    "df = df[[\"tag\", \"id\", \"classes\", \"attributes\", \"bounding_x\", \"bounding_y\", \"bounding_width\", \"bounding_height\", \"cssSelector\"]]\n",
    "\n",
    "# Encode target variable (CSS Selector)\n",
    "css_selector_encoder = LabelEncoder()\n",
    "df[\"cssSelector\"] = css_selector_encoder.fit_transform(df[\"cssSelector\"])\n",
    "\n",
    "# Combine textual features into a single string\n",
    "df[\"text\"] = df.apply(lambda row: f\"{row['tag']} {row['id']} {row['classes']} {row['attributes']}\", axis=1)\n",
    "\n",
    "# Tokenize input text\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer(df[\"text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Define features (X) and labels (Y)\n",
    "X = tokens[\"input_ids\"]\n",
    "y = df[\"cssSelector\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2538, 1012,  ..., 1012, 1014,  102],\n",
      "        [ 101, 2539, 1012,  ..., 1014,  102,    0],\n",
      "        [ 101, 2654, 1012,  ..., 1014,  102,    0],\n",
      "        ...,\n",
      "        [ 101, 2861, 1012,  ..., 1014,  102,    0],\n",
      "        [ 101, 1018, 1012,  ..., 1014,  102,    0],\n",
      "        [ 101, 2385, 1012,  ..., 1012, 1014,  102]])\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1983 2053 2077 ... 1992 1993 2052]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tag  id  classes  attributes  bounding_x   bounding_y  bounding_width  \\\n",
      "0      21   0       49         841    0.000000  -810.000000      572.666687   \n",
      "1      19   0        0           0    0.000000     0.000000        0.000000   \n",
      "2      28   0        0           2    0.000000     0.000000        0.000000   \n",
      "3      28   0        0         845    0.000000     0.000000        0.000000   \n",
      "4      28   0        0         844    0.000000     0.000000        0.000000   \n",
      "...   ...  ..      ...         ...         ...          ...             ...   \n",
      "2102   10   0       53          56   16.000000  1270.479248      540.666687   \n",
      "2103   10   0       52          55   16.000000  1270.479248      540.666687   \n",
      "2104   31   0       33          35   16.000000  1270.479248      540.666687   \n",
      "2105    4   0        0           0  461.229187  1272.479248        0.000000   \n",
      "2106   16   0        0         792    0.000000  1312.875000        0.000000   \n",
      "\n",
      "      bounding_height  cssSelector                 text  \n",
      "0         2122.875000         1983  21.0 0.0 49.0 841.0  \n",
      "1            0.000000         2053     19.0 0.0 0.0 0.0  \n",
      "2            0.000000         2077     28.0 0.0 0.0 2.0  \n",
      "3            0.000000         2081   28.0 0.0 0.0 845.0  \n",
      "4            0.000000         2082   28.0 0.0 0.0 844.0  \n",
      "...               ...          ...                  ...  \n",
      "2102        26.395834         1990   10.0 0.0 53.0 56.0  \n",
      "2103        26.395834         1991   10.0 0.0 52.0 55.0  \n",
      "2104        26.395834         1992   31.0 0.0 33.0 35.0  \n",
      "2105        21.333334         1993      4.0 0.0 0.0 0.0  \n",
      "2106         0.000000         2052   16.0 0.0 0.0 792.0  \n",
      "\n",
      "[2107 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1685, Testing size: 422\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Convert labels to Tensor format\n",
    "y = torch.tensor(y)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Move data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)\n",
    "\n",
    "print(f\"Training size: {len(X_train)}, Testing size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define Transformer-based Model\n",
    "class DOMSelectorModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DOMSelectorModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # Take [CLS] token output\n",
    "        return self.fc(cls_output)\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(css_selector_encoder.classes_)\n",
    "model = DOMSelectorModel(num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 7.714084184394692\n",
      "Epoch 2/5, Loss: 7.709784278329813\n",
      "Epoch 3/5, Loss: 7.630587978183098\n",
      "Epoch 4/5, Loss: 7.583169370327356\n",
      "Epoch 5/5, Loss: 7.563081628871414\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"dom_selector_transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOMSelectorModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=2107, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"dom_selector_transformer.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted CSS Selector: #check-scoring > dl > dd > dl > dd:nth-child(1) > dl > dd:nth-child(4) > div\n"
     ]
    }
   ],
   "source": [
    "def predict_css_selector(tag, element_id, classes, attributes):\n",
    "    text = f\"{tag} {element_id} {classes} {attributes}\"\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(tokens[\"input_ids\"])\n",
    "        pred = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    return css_selector_encoder.inverse_transform([pred])[0]\n",
    "\n",
    "# Example prediction\n",
    "predicted_selector = predict_css_selector(\"li\", \"check-scoring\", \"\", \"[{'name': 'class', 'value': 'reference internal'}]\")\n",
    "print(f\"Predicted CSS Selector: {predicted_selector}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
